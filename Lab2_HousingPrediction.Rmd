---
title: "Lab II: Housing Price Prediction"
author: "Zifeng Zhao"
date: "week 03 session 02"
output: html_document
editor_options: 
  chunk_output_type: console
---

In this lab, we will build linear regression, GAM and neural networks for predicting resale values of a house.

## 1. Data exploration and visualization
Let's read in the data from `WestRoxbury.csv` and further conduct some data visualization.
```{r chunk1}
total_data <- read.csv("./WestRoxbury.csv", header=T, stringsAsFactors=T)
str(total_data)
```

**Exercise 1** Create a scatter plot between `total_value` and `rooms`. Note that `rooms` is a numerical predictor but only takes a finite number of values. Thus, let's further make a boxplot between `total_value` and `rooms`, which is more informative.
```{r Q1}
plot(total_data$rooms, total_data$total_value)
boxplot(total_data$total_value~total_data$rooms)
```

**Exercise 2** Create a boxplot between `total_value` and `remodel`. Does remodeling help increase house value?
```{r Q2}

```

**Exercise 3**
Create a scatter plot between `total_value` and `yr_built`. Is there any non-linear effect? Similarly, create a scatter plot between `total_value` and `living_area` and investigate the relationship.
```{r Q3}

```

**Exercise 4** Plot the histogram of `total_value`. Do we need a log transformation? If so, further create a `log_total_value` variable in the `R` object `total_data` using the formula $\widetilde{Y}=\log(Y+1).$
```{r Q4}

```


## 2. Linear regression, GAM and NN
### 2.1 Data partition
**Exercise 5** In total, we have `r nrow(total_data)` observations, which is a decent sample size. Thus, let's partition the data in `total_data` into training **(70%)** and test data **(30%)** and store them as `R` objects `train_data` and `test_data` respectively. Use random seed **`set.seed(7)`**!

```{r Q5}
set.seed(7)
total_obs <- dim(total_data)[1]
train_indices <- sample(1:total_obs, 0.7*total_obs)
train_data <- total_data[train_indices,]
test_data <- total_data[-train_indices,]
```

### 2.2 Linear regression
**Exercise 6** Estimate a linear regression model with all **10** predictors `lot_sqft+yr_built+gross_area+living_area+floors+rooms+full_bath+half_bath+fireplace+remodel`
and name it `lm1`. Note the dependent variable should be `log_total_value`.
```{r Q6}
# lm1 <- 
  
```


### 2.3 Generalized additive model
**Exercise 7** Estimate a linear regression model with all **10** predictors `s(lot_sqft,df=4)+s(yr_built,df=4)+s(gross_area,df=4)+s(living_area,df=4)+floors+rooms+full_bath+half_bath+fireplace+remodel` and name it `lm1`. Note that for numerical predictors `lot_sqft`, `yr_built`, `gross_area` and `living_area`, we specify them as splines with degree-of-freedom=4.

```{r Q7}
library(gam)
# gam1 <- 

```

**Exercise 8** Plot the estimated coefficient functions for `gam1` using the function `plot()`. Does GAM capture the non-linear effect of numerical predictors `lot_sqft`, `yr_built`, `gross_area` and `living_area`?
```{r Q8}

```


### 2.4 Neural networks
Estimate an NN with all **10** predictors, name it `nn1`. For the architecture of NN, let's use two hidden layers with 4 hidden units in the first layer and 4 hidden units in the second layer.

**Exercise 9** Let's first generate the **training dataset** that are needed for the estimation of NN using the function `model.matrix()` and store it in `x_train_nn`. In addition, use the `scale()` function to standardize the predictors by centering with mean and scaling with sd. In addition, combine the `log_total_value` with the standardized predictors stored in `x_train_nn`. Make sure to rename the column name of `x_train_nn` correctly for `log_total_value`!

```{r Q9}
# x_train_nn <- 
x_train_nn <- model.matrix(~lot_sqft+yr_built+gross_area+living_area+floors+rooms+full_bath+half_bath+fireplace+remodel, data=train_data)[,-1]
x_mean <- apply(x_train_nn, 2, mean)
x_sd <- apply(x_train_nn, 2, sd)
x_train_nn <- scale(x_train_nn, center=x_mean, scale=x_sd)

x_train_nn <- cbind.data.frame(train_data$log_total_value, x_train_nn)
colnames(x_train_nn)[1] <- 'log_total_value'
```

**Exercise 10** Let's further generate the **test dataset** that are needed for the out-of-sample prediction evaluation of NN using the function `model.matrix()` and store it in `x_test_nn`. Use the `scale()` function to standardize the predictors by centering with mean and scaling with sd as in Exercise 9.
```{r Q10}
# x_test_nn <- 

```

**Exercise 11** Let's fit an NN that has two hidden layers with 4 hidden units in the first layer and 4 hidden units in the second layer. Make sure to use random seed **`set.seed(7)`**!
```{r Q11}
set.seed(7)
library(neuralnet)
# nn1 <- 
# plot(nn1)
```


### 2.5 Model evaluation (out-of-sample)
Let's now evaluate the prediction performance of the three statistical models `lm1`, `gam1` and `nn1` on the test data. First, let's generate the prediction by each model and store them in `lm1_pred`, `gam1_pred` and `nn1_pred` respectively. Make sure to transform the prediction back to the **original** scale. We then use the `accuracy()` function from the `forecast` package to obtain the error metrics.

```{r Q12}
library(forecast)
# lm1_pred <-
# gam1_pred <-
# nn1_pred <-


```

**Exercise 12** Which model should we choose and why?


### 3. Statistical analysis on the original-scale `total_value`
**Exercise 13** Let's estimate linear regression and GAM as before but with the **original scale** `total_value` and name them `lm2` and `gam2` respectively.

```{r Q13}
# lm2 and gam2
# lm2 <- 
# gam2 <- 
```

**Exercise 14** Let's estimate NN as before but with the **original scale** `total_value` and name them `nn2`. Note that for the original scale `total_value`, we indeed need to further standardize the dependent variable `total_value` by dividing its maximum value.

```{r Q14}
# generate training dataset for NN and standardize the predictors
# x_train_nn <- 

# standardize the dependent variable total_value for numerical stability

# generate test dataset for NN and standardize the predictors
# x_test_nn <- 

# estimate NN
set.seed(7)
# nn2 <- 
```

Let's further conduct model evaluation on the test data.
```{r Model evaluation}
# lm2_pred <- 
# gam2_pred <- 
# nn2_pred <-

```

