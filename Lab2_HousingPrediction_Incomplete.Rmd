---
title: "Lab II: Housing Price Prediction"
author: "Vijay Penmetsa"
date: "week 03 session 02"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
library(ggplot2)
```

In this lab, we will build linear regression, GAM and neural networks for predicting resale values of a house.

## 1. Data exploration and visualization
Let's read in the data from `WestRoxbury.csv` and further conduct some data visualization.
```{r chunk1}
total_data <- read.csv("./WestRoxbury.csv", header=T, stringsAsFactors=T)
str(total_data)
```

**Exercise 1** Create a scatter plot between `total_value` and `rooms`. Note that `rooms` is a numerical predictor but only takes a finite number of values. Thus, let's further make a boxplot between `total_value` and `rooms`, which is more informative.
```{r Q1}
plot(total_data$rooms, total_data$total_value)
boxplot(total_value ~ rooms, data = total_data)
```
  
Boxplot is more informative.  
The price increases with increasing number of rooms until homes with 12 rooms. Post that, the increase in room numbers negatively affect price.
  
**Exercise 2** Create a boxplot between `total_value` and `remodel`. Does remodeling help increase house value?
```{r Q2}
boxplot(total_value ~ remodel, data = total_data)
```
  
Remodelling helps increase the value of the house but not by a large margin.
  
**Exercise 3**
Create a scatter plot between `total_value` and `yr_built`. Is there any non-linear effect? Similarly, create a scatter plot between `total_value` and `living_area` and investigate the relationship.
```{r Q3a}
ggplot(total_data, aes(x = yr_built, y = total_value))+
  geom_point(alpha = 0.3)+
  theme_minimal()
```
  
The relationship between total_value and yr_built is non linear.

```{r Q3b}
ggplot(total_data, aes(x = living_area, y = total_value))+
  geom_point(alpha = 0.3)+
  theme_minimal()
```
  
The relationship between total_value and living_area is linear.
  

**Exercise 4** Plot the histogram of `total_value`. Do we need a log transformation? If so, further create a `log_total_value` variable in the `R` object `total_data` using the formula $\widetilde{Y}=\log(Y+1).$
```{r Q4a}
hist(total_data$total_value)
```
  
We need log transformation to fix the positive skew.

```{r Q4b}
total_data$log_total_value <- log(total_data$total_value + 1)

hist(total_data$log_total_value)
```


## 2. Linear regression, GAM and NN
### 2.1 Data partition
**Exercise 5** In total, we have `r nrow(total_data)` observations, which is a decent sample size. Thus, let's partition the data in `total_data` into training **(70%)** and test data **(30%)** and store them as `R` objects `train_data` and `test_data` respectively. Use random seed **`set.seed(7)`**!

```{r Q5}
set.seed(7)
total_obs <- dim(total_data)[1]

train_data_indices <- sample(1:total_obs, 0.7*total_obs)

train_data <- total_data[train_data_indices,]
test_data <- total_data[-train_data_indices,]

train_obs <- dim(train_data)[1]
test_obs <- dim(test_data)[1]
```

### 2.2 Linear regression
**Exercise 6** Estimate a linear regression model with all **10** predictors `lot_sqft+yr_built+gross_area+living_area+floors+rooms+full_bath+half_bath+fireplace+remodel`
and name it `lm1`. Note the dependent variable should be `log_total_value`.
```{r Q6}
lm1 <- lm(log_total_value ~ 
            lot_sqft
          + yr_built
          + gross_area
          + living_area
          + floors
          + rooms
          + full_bath
          + half_bath
          + fireplace
          + remodel, 
          data = train_data)

summary(lm1)
  
```


### 2.3 Generalized additive model
**Exercise 7** Estimate a linear regression model with all **10** predictors `s(lot_sqft,df=4)+s(yr_built,df=4)+s(gross_area,df=4)+s(living_area,df=4)+floors+rooms+full_bath+half_bath+fireplace+remodel` and name it `lm1`. Note that for numerical predictors `lot_sqft`, `yr_built`, `gross_area` and `living_area`, we specify them as splines with degree-of-freedom=4.

```{r Q7}
library(gam)
gam1 <- gam(log_total_value ~ 
              s(lot_sqft,df=4) 
            + s(yr_built,df=4)
            + s(gross_area,df=4)
            + s(living_area,df=4)
            + floors
            + rooms
            + full_bath
            + half_bath
            + fireplace
            + remodel, 
            data = train_data)

#summary(gam1)
```

**Exercise 8** Plot the estimated coefficient functions for `gam1` using the function `plot()`. Does GAM capture the non-linear effect of numerical predictors `lot_sqft`, `yr_built`, `gross_area` and `living_area`?
```{r Q8}
plot(gam1)
```
  
Yes. GAM captures the non-linear effect of the four numerical predictors.
  

### 2.4 Neural networks
Estimate an NN with all **10** predictors, name it `nn1`. For the architecture of NN, let's use two hidden layers with 4 hidden units in the first layer and 4 hidden units in the second layer.

**Exercise 9** Let's first generate the **training dataset** that are needed for the estimation of NN using the function `model.matrix()` and store it in `x_train_nn`. In addition, use the `scale()` function to standardize the predictors by centering with mean and scaling with sd. In addition, combine the `log_total_value` with the standardized predictors stored in `x_train_nn`. Make sure to rename the column name of `x_train_nn` correctly for `log_total_value`!

```{r Q9}
#x_train_nn <- 
x_train_nn <- model.matrix(~lot_sqft
                           +yr_built
                           +gross_area
                           +living_area
                           +floors
                           +rooms
                           +full_bath
                           +half_bath
                           +fireplace
                           +remodel
                           , data=train_data)[,-1]


#x_train_nn <- x_train_nn[,-1]

# standardization
x_mean <- apply(x_train_nn, 2, mean) # 2 means columnwise
x_sd <- apply(x_train_nn, 2, sd)

x_train_nn <- scale(x_train_nn, center=x_mean, scale=x_sd)

# combine with dependent variable Log_price
x_train_nn <- cbind.data.frame(train_data$log_total_value, x_train_nn)
colnames(x_train_nn)[1] <- 'log_total_value'  

```

**Exercise 10** Let's further generate the **test dataset** that are needed for the out-of-sample prediction evaluation of NN using the function `model.matrix()` and store it in `x_test_nn`. Use the `scale()` function to standardize the predictors by centering with mean and scaling with sd as in Exercise 9.
```{r Q10}
# x_test_nn <- 

x_test_nn <- model.matrix(~lot_sqft+yr_built+gross_area+living_area+floors+rooms+full_bath+half_bath+fireplace+remodel, data=test_data)[,-1]

#x_test_nn <- x_test_nn[,-1]
# standardization
x_test_nn <- scale(x_test_nn, center = x_mean, scale = x_sd)

# combine with dependent variable Log_price
x_test_nn <- cbind.data.frame(test_data$log_total_value, x_test_nn)
colnames(x_test_nn)[1] <- 'log_total_value'

```

**Exercise 11** Let's fit an NN that has two hidden layers with 4 hidden units in the first layer and 4 hidden units in the second layer. Make sure to use random seed **`set.seed(7)`**!
```{r Q11}
set.seed(7)
library(neuralnet)

nn1 <- neuralnet(log_total_value~lot_sqft+yr_built+gross_area+living_area+floors+rooms+full_bath+half_bath+fireplace+remodelOld+remodelRecent, data=x_train_nn, hidden=c(4,4))

plot(nn1, rep="best")
```


### 2.5 Model evaluation (out-of-sample)
Let's now evaluate the prediction performance of the three statistical models `lm1`, `gam1` and `nn1` on the test data. First, let's generate the prediction by each model and store them in `lm1_pred`, `gam1_pred` and `nn1_pred` respectively. Make sure to transform the prediction back to the **original** scale. We then use the `accuracy()` function from the `forecast` package to obtain the error metrics.

```{r Q12}
library(forecast)
lm1_pred <- exp(predict(lm1, newdata = test_data)) - 1
gam1_pred <- exp(predict(gam1, newdata = test_data)) - 1
nn1_pred <- exp(predict(nn1, newdata = x_test_nn)[,1]) - 1

accuracy(lm1_pred, test_data$total_value)
accuracy(gam1_pred, test_data$total_value)
accuracy(nn1_pred, test_data$total_value)

```

**Exercise 12** Which model should we choose and why?
We should chose GAM. It has better MAE and is simpler.

### 3. Statistical analysis on the original-scale `total_value`
**Exercise 13** Let's estimate linear regression and GAM as before but with the **original scale** `total_value` and name them `lm2` and `gam2` respectively.

```{r Q13}
# lm2 and gam2
lm2 <- lm(total_value ~ 
            lot_sqft
          + yr_built
          + gross_area
          + living_area
          + floors
          + rooms
          + full_bath
          + half_bath
          + fireplace
          + remodel, 
          data = train_data)

gam2 <- gam(total_value ~ 
              s(lot_sqft,df=4) 
            + s(yr_built,df=4)
            + s(gross_area,df=4)
            + s(living_area,df=4)
            + floors
            + rooms
            + full_bath
            + half_bath
            + fireplace
            + remodel, 
            data = train_data)

```

**Exercise 14** Let's estimate NN as before but with the **original scale** `total_value` and name them `nn2`. Note that for the original scale `total_value`, we indeed need to further standardize the dependent variable `total_value` by dividing its maximum value.

```{r Q14}
# generate training dataset for NN and standardize the predictors
x_train_nn <- model.matrix(~lot_sqft+yr_built+gross_area+living_area+floors+rooms+full_bath+half_bath+fireplace+remodel, data=train_data)[,-1]

# standardize the dependent variable total_value for numerical stability
x_mean <- apply(x_train_nn, 2, mean) # 2 means columnwise
x_sd <- apply(x_train_nn, 2, sd)

x_train_nn <- scale(x_train_nn, center=x_mean, scale=x_sd)

# combine with dependent variable Log_price
y_max <- max(train_data$total_value)
x_train_nn <- cbind.data.frame(train_data$total_value/y_max, x_train_nn)
colnames(x_train_nn)[1] <- 'st_total_value'  
# generate test dataset for NN and standardize the predictors
# x_test_nn <- 

x_test_nn <- model.matrix(~lot_sqft+yr_built+gross_area+living_area+floors+rooms+full_bath+half_bath+fireplace+remodel, data=test_data)[,-1]
x_test_nn <- scale(x_test_nn, center=x_mean, scale=x_sd)


# estimate NN
set.seed(7)
nn2 <- neuralnet(st_total_value~lot_sqft+yr_built+gross_area+living_area+floors+rooms+full_bath+half_bath+fireplace+remodelOld+remodelRecent, data=x_train_nn, hidden=c(4,4)) 

```

Let's further conduct model evaluation on the test data.
```{r Model evaluation}
# lm2_pred <- 
# gam2_pred <- 
# nn2_pred <-
lm2_pred <- predict(lm2, newdata=test_data)
gam2_pred <- predict(gam2, newdata=test_data)
nn2_pred <- predict(nn2, newdata=x_test_nn, type='response')[,1]*y_max

accuracy(lm2_pred, test_data$total_value)
accuracy(gam2_pred, test_data$total_value)
accuracy(nn2_pred, test_data$total_value)

```

